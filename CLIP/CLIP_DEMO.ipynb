{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e6af39-32d5-4dd9-9d7c-bdbbb43d0742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import numpy as np\n",
    "\n",
    "# 모델 로드\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model.eval()\n",
    "\n",
    "# Oxford-IIIT Pet 로드\n",
    "pet_dataset = OxfordIIITPet(\n",
    "    root=os.path.expanduser(\"~/.cache\"),\n",
    "    split='test',\n",
    "    target_types='category',\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# 무작위 이미지 선택\n",
    "indices = torch.randperm(len(pet_dataset))[:2000].tolist()\n",
    "images = [pet_dataset[i][0] for i in indices]\n",
    "gt_labels = [pet_dataset.classes[pet_dataset[i][1]] for i in indices]\n",
    "\n",
    "# 1. SINGLE PROMPT (BASELINE)\n",
    "print(\"\\nSINGLE PROMPT EVALUATION\")\n",
    "single_template = [\"a photo of a {}\"]\n",
    "single_texts_per_class = [[t.format(c) for t in single_template] for c in pet_dataset.classes]\n",
    "single_flat_texts = [t for texts in single_texts_per_class for t in texts]\n",
    "\n",
    "# Single prompt CLIP 입력\n",
    "single_inputs = processor(\n",
    "    images=images,\n",
    "    text=single_flat_texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    single_outputs = model(**single_inputs)\n",
    "    single_image_features = single_outputs.image_embeds\n",
    "    single_text_features = single_outputs.text_embeds\n",
    "\n",
    "# Single 정규화\n",
    "single_image_features = single_image_features / single_image_features.norm(dim=-1, keepdim=True)\n",
    "single_text_features = single_text_features / single_text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Single 클래스별 평균 (37, 1, 512) → (37, 512)\n",
    "num_classes = len(pet_dataset.classes)\n",
    "single_text_features = single_text_features.view(num_classes, 1, -1).mean(dim=1)\n",
    "single_similarity = single_image_features @ single_text_features.T\n",
    "\n",
    "# Single 성능 계산\n",
    "similarity_topk = 2\n",
    "single_values, single_indices_pred = single_similarity.topk(similarity_topk, dim=1)\n",
    "\n",
    "single_correct = 0\n",
    "single_margins = []\n",
    "for img_idx in range(len(images)):\n",
    "    gt_idx = pet_dataset.class_to_idx[gt_labels[img_idx]]\n",
    "    if single_indices_pred[img_idx, 0] == gt_idx:\n",
    "        single_correct += 1\n",
    "    if similarity_topk > 1:\n",
    "        margin = (single_values[img_idx, 0] - single_values[img_idx, 1]).item()\n",
    "        single_margins.append(margin)\n",
    "\n",
    "single_acc = single_correct / len(images) * 100\n",
    "single_avg_margin = np.mean(single_margins) if single_margins else 0\n",
    "\n",
    "# 2. 템플릿 앙상블\n",
    "print(\"\\nTEMPLATE ENSEMBLE EVALUATION\")\n",
    "# Oxford-IIIT Pet 및 동물 분류에 최적화된 10대 템플릿\n",
    "templates = [\n",
    "    \"a photo of a {}, a type of pet.\",               # 가장 강력한 기본형\n",
    "    \"a photo of the {}, a type of cat or dog.\",      # 대분류(개/고양이) 명시\n",
    "    \"a photo of a {}, a breed of dog.\",              # 품종 맥락 추가\n",
    "    \"a close-up photo of a {}.\",                     # 근접 촬영 대응\n",
    "    \"a photo of a sitting {}.\",                      # 자세 정보 추가\n",
    "    \"a pet portrait of a {}.\",                       # 인물화 형식의 구도\n",
    "    \"the {} is shown in the image.\",                 # 객체 중심 설명\n",
    "    \"a blurry photo of a {}.\",                       # 저화질/노이즈 대응\n",
    "    \"a photo of a {} looking at the camera.\",        # 시선 처리 대응\n",
    "    \"a high quality photo of a {}.\"                  # 고화질 특징 강조\n",
    "]\n",
    "\n",
    "num_templates = len(templates)\n",
    "texts_per_class = [[template.format(c) for template in templates] for c in pet_dataset.classes]\n",
    "flat_texts = [t for texts in texts_per_class for t in texts]\n",
    "\n",
    "# Ensemble CLIP 입력\n",
    "inputs = processor(\n",
    "    images=images, \n",
    "    text=flat_texts, \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    image_features = outputs.image_embeds\n",
    "    text_features = outputs.text_embeds\n",
    "\n",
    "# Ensemble 정규화 & 평균\n",
    "image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "text_features = text_features.view(num_classes, num_templates, -1).mean(dim=1)\n",
    "ensemble_similarity = image_features @ text_features.T\n",
    "\n",
    "# Ensemble 성능 계산\n",
    "ensemble_values, ensemble_indices_pred = ensemble_similarity.topk(similarity_topk, dim=1)\n",
    "ensemble_correct = 0\n",
    "ensemble_margins = []\n",
    "for img_idx in range(len(images)):\n",
    "    gt_idx = pet_dataset.class_to_idx[gt_labels[img_idx]]\n",
    "    if ensemble_indices_pred[img_idx, 0] == gt_idx:\n",
    "        ensemble_correct += 1\n",
    "    if similarity_topk > 1:\n",
    "        margin = (ensemble_values[img_idx, 0] - ensemble_values[img_idx, 1]).item()\n",
    "        ensemble_margins.append(margin)\n",
    "\n",
    "ensemble_acc = ensemble_correct / len(images) * 100\n",
    "ensemble_avg_margin = np.mean(ensemble_margins) if ensemble_margins else 0\n",
    "\n",
    "#최종 비교 결과\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"SINGLE PROMPT vs {len(templates)} TEMPLATE ENSEMBLE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Metric':<25} {'Single':<12} {'Ensemble':<12} {'Improvement':<12}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Top-1 Accuracy     : {single_acc:6.1f}% ({single_correct:3d})  {ensemble_acc:6.1f}% ({ensemble_correct:3d})  {ensemble_acc-single_acc:+6.1f}%\")\n",
    "print(f\"Avg Margin         : {single_avg_margin:8.3f}       {ensemble_avg_margin:8.3f}       {ensemble_avg_margin-single_avg_margin:+7.3f}\")\n",
    "print(f\"Improvement Rate   : {'':<18} {'':<12} {((ensemble_correct/single_correct-1)*100 if single_correct>0 else 0):+6.1f}%\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 상세 결과 (첫 10개)\n",
    "print(\"\\nDETAILED RESULTS (first 5 images):\")\n",
    "for img_idx in range(min(5, len(images))):\n",
    "    gt_idx = pet_dataset.class_to_idx[gt_labels[img_idx]]\n",
    "    single_correct = \"O\" if single_indices_pred[img_idx, 0] == gt_idx else \"X\"\n",
    "    ensemble_correct = \"O\" if ensemble_indices_pred[img_idx, 0] == gt_idx else \"X\"\n",
    "    \n",
    "    print(f\"\\nImage {img_idx+1} | Answer: {gt_labels[img_idx]}\")\n",
    "    print(f\"  Single:  {pet_dataset.classes[single_indices_pred[img_idx, 0]]} {single_correct} ({single_values[img_idx, 0]:.3f})\")\n",
    "    print(f\"  Ensemble:{pet_dataset.classes[ensemble_indices_pred[img_idx, 0]]} {ensemble_correct} ({ensemble_values[img_idx, 0]:.3f})\")\n",
    "\n",
    "# 이미지 시각화\n",
    "n_show = min(8, len(images))\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (ax, img, gt) in enumerate(zip(axes, images[:n_show], gt_labels[:n_show])):\n",
    "    ax.imshow(img)\n",
    "    gt_idx = pet_dataset.class_to_idx[gt]\n",
    "    single_pred = pet_dataset.classes[single_indices_pred[idx, 0]]\n",
    "    ensemble_pred = pet_dataset.classes[ensemble_indices_pred[idx, 0]]\n",
    "    single_status = \"O\" if single_indices_pred[idx, 0] == gt_idx else \"X\"\n",
    "    ensemble_status = \"O\" if ensemble_indices_pred[idx, 0] == gt_idx else \"X\"\n",
    "    \n",
    "    title = f\"Answer: {gt}\\nSingle: {single_pred} {single_status}\\nEnsemble: {ensemble_pred} {ensemble_status}\"\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(f'Single Prompt vs {len(templates)} Prompt', fontsize=17)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11284e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
